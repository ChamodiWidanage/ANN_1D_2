{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cbb7fRy-eyr"
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sNDnxE2-pwE"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxChR1Rk-umf",
    "ExecuteTime": {
     "end_time": "2024-02-06T00:00:36.992042200Z",
     "start_time": "2024-02-06T00:00:30.285801200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AG3FQEch-yuA"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-4zq8Mza_D9O"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9CV13Co_HHM",
    "ExecuteTime": {
     "end_time": "2024-02-06T00:00:43.819905900Z",
     "start_time": "2024-02-06T00:00:43.428251800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 588 entries, 0 to 587\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Explosive type          588 non-null    object \n",
      " 1   Explosive mass          588 non-null    float64\n",
      " 2   Standoff distance       588 non-null    float64\n",
      " 3   Peak incident pressure  588 non-null    float64\n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 18.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_excel('PDataset4.xlsx')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T00:00:45.182122400Z",
     "start_time": "2024-02-06T00:00:45.154254900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 588 entries, 0 to 587\n",
      "Data columns (total 5 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Explosive mass                588 non-null    float64\n",
      " 1   Standoff distance             588 non-null    float64\n",
      " 2   Peak incident pressure        588 non-null    float64\n",
      " 3   Explosive type_Composition B  588 non-null    uint8  \n",
      " 4   Explosive type_TNT            588 non-null    uint8  \n",
      "dtypes: float64(3), uint8(2)\n",
      "memory usage: 15.1 KB\n"
     ]
    }
   ],
   "source": [
    "# convert categorical variable into dummy variables\n",
    "dataset = pd.get_dummies(dataset, columns=['Explosive type'])\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T00:00:46.795411500Z",
     "start_time": "2024-02-06T00:00:46.737625400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Explosive mass  Standoff distance  Peak incident pressure  \\\n0             0.5                1.0                 597.460   \n1             0.5                1.5                 283.258   \n2             0.5                2.5                 163.904   \n3             0.5                3.5                 135.678   \n4             0.5                4.5                 124.039   \n\n   Explosive type_Composition B  Explosive type_TNT  \n0                             0                   1  \n1                             0                   1  \n2                             0                   1  \n3                             0                   1  \n4                             0                   1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Explosive mass</th>\n      <th>Standoff distance</th>\n      <th>Peak incident pressure</th>\n      <th>Explosive type_Composition B</th>\n      <th>Explosive type_TNT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.5</td>\n      <td>1.0</td>\n      <td>597.460</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.5</td>\n      <td>1.5</td>\n      <td>283.258</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.5</td>\n      <td>2.5</td>\n      <td>163.904</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.5</td>\n      <td>3.5</td>\n      <td>135.678</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.5</td>\n      <td>4.5</td>\n      <td>124.039</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T00:00:48.199653Z",
     "start_time": "2024-02-06T00:00:48.171899100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(588, 4) (588,)\n"
     ]
    }
   ],
   "source": [
    "y = dataset[('Peak incident pressure')]\n",
    "X = dataset.drop('Peak incident pressure', axis=1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T00:00:49.483713Z",
     "start_time": "2024-02-06T00:00:49.455123500Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VC6omXel_Up0"
   },
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5edeb2r_agx",
    "ExecuteTime": {
     "end_time": "2024-02-06T00:00:51.938855900Z",
     "start_time": "2024-02-06T00:00:51.911324800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_test,\n",
    "                                                y_test,\n",
    "                                                test_size = 0.5,\n",
    "                                                random_state = 71)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T00:00:53.108163Z",
     "start_time": "2024-02-06T00:00:53.074223100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature scaling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:,0:2] = sc.fit_transform(X_train[:, 0:2])\n",
    "print (X_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test[:,0:2] = sc.transform(X_test[:, 0:2])\n",
    "print (X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_val[:,0:2] = sc.transform(X_val[:, 0:2])\n",
    "print (X_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1 -Hyperparameter tuning - neurons, activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T01:01:56.466029800Z",
     "start_time": "2023-12-27T01:01:56.465525300Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "# Set seed for NumPy\n",
    "np.random.seed(71)\n",
    "\n",
    "# Set seed for TensorFlow\n",
    "tf.random.set_seed(71)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "def create_model1(last_layer_nodes, activation_func):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 200,  input_shape=(X_train.shape[1],), activation=activation_func))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(last_layer_nodes, activation=activation_func))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    optimizer1 = Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer = optimizer1, loss = 'mean_squared_error', metrics = ['mae'])\n",
    "    return model\n",
    "\n",
    "activation_func = ['relu', 'leaky_relu', 'softplus']\n",
    "last_layer_nodes = [10, 20, 30, 40, 50,60, 70, 80,90,100,110,120,130, 140, 150,160, 170, 180, 190, 200]\n",
    "\n",
    "param_grid = dict( model__activation_func = activation_func,model__last_layer_nodes = last_layer_nodes)\n",
    "\n",
    "##Wrap model into scikit-learn\n",
    "model1 = KerasRegressor(model=create_model1, verbose=0, epochs = 100, batch_size = 50, random_state = 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T01:02:53.244880100Z",
     "start_time": "2023-12-27T01:01:59.059967600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.996222 using {'model__activation_func': 'softplus', 'model__last_layer_nodes': 200}\n",
      "0.971583 (0.013325) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 10}\n",
      "0.977863 (0.010218) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 20}\n",
      "0.983471 (0.006943) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 30}\n",
      "0.985640 (0.009240) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 40}\n",
      "0.985361 (0.007294) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 50}\n",
      "0.981960 (0.010313) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 60}\n",
      "0.993383 (0.002968) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 70}\n",
      "0.992613 (0.005188) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 80}\n",
      "0.991611 (0.005634) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 90}\n",
      "0.986637 (0.004731) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 100}\n",
      "0.991704 (0.005330) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 110}\n",
      "0.989765 (0.006015) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 120}\n",
      "0.989459 (0.005579) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 130}\n",
      "0.988202 (0.005517) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 140}\n",
      "0.987503 (0.006388) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 150}\n",
      "0.987325 (0.005849) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 160}\n",
      "0.990391 (0.005763) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 170}\n",
      "0.990206 (0.006629) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 180}\n",
      "0.988958 (0.005469) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 190}\n",
      "0.983430 (0.007187) with: {'model__activation_func': 'relu', 'model__last_layer_nodes': 200}\n",
      "0.967085 (0.012938) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 10}\n",
      "0.970262 (0.010271) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 20}\n",
      "0.972607 (0.010045) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 30}\n",
      "0.974339 (0.009165) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 40}\n",
      "0.975078 (0.009278) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 50}\n",
      "0.973357 (0.010191) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 60}\n",
      "0.975311 (0.009584) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 70}\n",
      "0.974660 (0.010261) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 80}\n",
      "0.974273 (0.010734) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 90}\n",
      "0.973387 (0.012022) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 100}\n",
      "0.974594 (0.013380) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 110}\n",
      "0.974992 (0.013143) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 120}\n",
      "0.973733 (0.012158) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 130}\n",
      "0.973098 (0.013649) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 140}\n",
      "0.973142 (0.012691) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 150}\n",
      "0.972406 (0.016208) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 160}\n",
      "0.972478 (0.014279) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 170}\n",
      "0.973169 (0.012363) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 180}\n",
      "0.976556 (0.012481) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 190}\n",
      "0.973669 (0.014856) with: {'model__activation_func': 'leaky_relu', 'model__last_layer_nodes': 200}\n",
      "0.968304 (0.013919) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 10}\n",
      "0.968578 (0.014122) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 20}\n",
      "0.976199 (0.015614) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 30}\n",
      "0.985881 (0.008965) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 40}\n",
      "0.984561 (0.009254) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 50}\n",
      "0.990891 (0.004753) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 60}\n",
      "0.991503 (0.005131) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 70}\n",
      "0.992468 (0.004351) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 80}\n",
      "0.990911 (0.007081) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 90}\n",
      "0.991529 (0.009247) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 100}\n",
      "0.990364 (0.010463) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 110}\n",
      "0.990709 (0.010872) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 120}\n",
      "0.991264 (0.009611) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 130}\n",
      "0.991197 (0.011520) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 140}\n",
      "0.991263 (0.010863) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 150}\n",
      "0.991056 (0.011993) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 160}\n",
      "0.991857 (0.011120) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 170}\n",
      "0.995169 (0.005519) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 180}\n",
      "0.994680 (0.006095) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 190}\n",
      "0.996222 (0.003486) with: {'model__activation_func': 'softplus', 'model__last_layer_nodes': 200}\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=71)\n",
    "grid1 = GridSearchCV(estimator = model1, param_grid= param_grid, n_jobs=-1, scoring = 'r2', cv=kf)\n",
    "grid_result1 = grid1.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result1.best_score_, grid_result1.best_params_))\n",
    "means = grid_result1.cv_results_['mean_test_score']\n",
    "stds = grid_result1.cv_results_['std_test_score']\n",
    "params = grid_result1.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "   model__activation_func  model__last_layer_nodes  model__first_layer_nodes  \\\n0                    relu                       10                       200   \n1                    relu                       20                       200   \n2                    relu                       30                       200   \n3                    relu                       40                       200   \n4                    relu                       50                       200   \n5                    relu                       60                       200   \n6                    relu                       70                       200   \n7                    relu                       80                       200   \n8                    relu                       90                       200   \n9                    relu                      100                       200   \n10                   relu                      110                       200   \n11                   relu                      120                       200   \n12                   relu                      130                       200   \n13                   relu                      140                       200   \n14                   relu                      150                       200   \n15                   relu                      160                       200   \n16                   relu                      170                       200   \n17                   relu                      180                       200   \n18                   relu                      190                       200   \n19                   relu                      200                       200   \n20             leaky_relu                       10                       200   \n21             leaky_relu                       20                       200   \n22             leaky_relu                       30                       200   \n23             leaky_relu                       40                       200   \n24             leaky_relu                       50                       200   \n25             leaky_relu                       60                       200   \n26             leaky_relu                       70                       200   \n27             leaky_relu                       80                       200   \n28             leaky_relu                       90                       200   \n29             leaky_relu                      100                       200   \n30             leaky_relu                      110                       200   \n31             leaky_relu                      120                       200   \n32             leaky_relu                      130                       200   \n33             leaky_relu                      140                       200   \n34             leaky_relu                      150                       200   \n35             leaky_relu                      160                       200   \n36             leaky_relu                      170                       200   \n37             leaky_relu                      180                       200   \n38             leaky_relu                      190                       200   \n39             leaky_relu                      200                       200   \n40               softplus                       10                       200   \n41               softplus                       20                       200   \n42               softplus                       30                       200   \n43               softplus                       40                       200   \n44               softplus                       50                       200   \n45               softplus                       60                       200   \n46               softplus                       70                       200   \n47               softplus                       80                       200   \n48               softplus                       90                       200   \n49               softplus                      100                       200   \n50               softplus                      110                       200   \n51               softplus                      120                       200   \n52               softplus                      130                       200   \n53               softplus                      140                       200   \n54               softplus                      150                       200   \n55               softplus                      160                       200   \n56               softplus                      170                       200   \n57               softplus                      180                       200   \n58               softplus                      190                       200   \n59               softplus                      200                       200   \n\n          R2  \n0   0.971583  \n1   0.977863  \n2   0.983471  \n3   0.985640  \n4   0.985361  \n5   0.981960  \n6   0.993383  \n7   0.992613  \n8   0.991611  \n9   0.986637  \n10  0.991704  \n11  0.989765  \n12  0.989459  \n13  0.988202  \n14  0.987503  \n15  0.987325  \n16  0.990391  \n17  0.990206  \n18  0.988958  \n19  0.983430  \n20  0.967085  \n21  0.970262  \n22  0.972607  \n23  0.974339  \n24  0.975078  \n25  0.973357  \n26  0.975311  \n27  0.974660  \n28  0.974273  \n29  0.973387  \n30  0.974594  \n31  0.974992  \n32  0.973733  \n33  0.973098  \n34  0.973142  \n35  0.972406  \n36  0.972478  \n37  0.973169  \n38  0.976556  \n39  0.973669  \n40  0.968304  \n41  0.968578  \n42  0.976199  \n43  0.985881  \n44  0.984561  \n45  0.990891  \n46  0.991503  \n47  0.992468  \n48  0.990911  \n49  0.991529  \n50  0.990364  \n51  0.990709  \n52  0.991264  \n53  0.991197  \n54  0.991263  \n55  0.991056  \n56  0.991857  \n57  0.995169  \n58  0.994680  \n59  0.996222  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model__activation_func</th>\n      <th>model__last_layer_nodes</th>\n      <th>model__first_layer_nodes</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>relu</td>\n      <td>10</td>\n      <td>200</td>\n      <td>0.971583</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>relu</td>\n      <td>20</td>\n      <td>200</td>\n      <td>0.977863</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>relu</td>\n      <td>30</td>\n      <td>200</td>\n      <td>0.983471</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>relu</td>\n      <td>40</td>\n      <td>200</td>\n      <td>0.985640</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>relu</td>\n      <td>50</td>\n      <td>200</td>\n      <td>0.985361</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>relu</td>\n      <td>60</td>\n      <td>200</td>\n      <td>0.981960</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>relu</td>\n      <td>70</td>\n      <td>200</td>\n      <td>0.993383</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>relu</td>\n      <td>80</td>\n      <td>200</td>\n      <td>0.992613</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>relu</td>\n      <td>90</td>\n      <td>200</td>\n      <td>0.991611</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>relu</td>\n      <td>100</td>\n      <td>200</td>\n      <td>0.986637</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>relu</td>\n      <td>110</td>\n      <td>200</td>\n      <td>0.991704</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>relu</td>\n      <td>120</td>\n      <td>200</td>\n      <td>0.989765</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>relu</td>\n      <td>130</td>\n      <td>200</td>\n      <td>0.989459</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>relu</td>\n      <td>140</td>\n      <td>200</td>\n      <td>0.988202</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>relu</td>\n      <td>150</td>\n      <td>200</td>\n      <td>0.987503</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>relu</td>\n      <td>160</td>\n      <td>200</td>\n      <td>0.987325</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>relu</td>\n      <td>170</td>\n      <td>200</td>\n      <td>0.990391</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>relu</td>\n      <td>180</td>\n      <td>200</td>\n      <td>0.990206</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>relu</td>\n      <td>190</td>\n      <td>200</td>\n      <td>0.988958</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>relu</td>\n      <td>200</td>\n      <td>200</td>\n      <td>0.983430</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>leaky_relu</td>\n      <td>10</td>\n      <td>200</td>\n      <td>0.967085</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>leaky_relu</td>\n      <td>20</td>\n      <td>200</td>\n      <td>0.970262</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>leaky_relu</td>\n      <td>30</td>\n      <td>200</td>\n      <td>0.972607</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>leaky_relu</td>\n      <td>40</td>\n      <td>200</td>\n      <td>0.974339</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>leaky_relu</td>\n      <td>50</td>\n      <td>200</td>\n      <td>0.975078</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>leaky_relu</td>\n      <td>60</td>\n      <td>200</td>\n      <td>0.973357</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>leaky_relu</td>\n      <td>70</td>\n      <td>200</td>\n      <td>0.975311</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>leaky_relu</td>\n      <td>80</td>\n      <td>200</td>\n      <td>0.974660</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>leaky_relu</td>\n      <td>90</td>\n      <td>200</td>\n      <td>0.974273</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>leaky_relu</td>\n      <td>100</td>\n      <td>200</td>\n      <td>0.973387</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>leaky_relu</td>\n      <td>110</td>\n      <td>200</td>\n      <td>0.974594</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>leaky_relu</td>\n      <td>120</td>\n      <td>200</td>\n      <td>0.974992</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>leaky_relu</td>\n      <td>130</td>\n      <td>200</td>\n      <td>0.973733</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>leaky_relu</td>\n      <td>140</td>\n      <td>200</td>\n      <td>0.973098</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>leaky_relu</td>\n      <td>150</td>\n      <td>200</td>\n      <td>0.973142</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>leaky_relu</td>\n      <td>160</td>\n      <td>200</td>\n      <td>0.972406</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>leaky_relu</td>\n      <td>170</td>\n      <td>200</td>\n      <td>0.972478</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>leaky_relu</td>\n      <td>180</td>\n      <td>200</td>\n      <td>0.973169</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>leaky_relu</td>\n      <td>190</td>\n      <td>200</td>\n      <td>0.976556</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>leaky_relu</td>\n      <td>200</td>\n      <td>200</td>\n      <td>0.973669</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>softplus</td>\n      <td>10</td>\n      <td>200</td>\n      <td>0.968304</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>softplus</td>\n      <td>20</td>\n      <td>200</td>\n      <td>0.968578</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>softplus</td>\n      <td>30</td>\n      <td>200</td>\n      <td>0.976199</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>softplus</td>\n      <td>40</td>\n      <td>200</td>\n      <td>0.985881</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>softplus</td>\n      <td>50</td>\n      <td>200</td>\n      <td>0.984561</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>softplus</td>\n      <td>60</td>\n      <td>200</td>\n      <td>0.990891</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>softplus</td>\n      <td>70</td>\n      <td>200</td>\n      <td>0.991503</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>softplus</td>\n      <td>80</td>\n      <td>200</td>\n      <td>0.992468</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>softplus</td>\n      <td>90</td>\n      <td>200</td>\n      <td>0.990911</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>softplus</td>\n      <td>100</td>\n      <td>200</td>\n      <td>0.991529</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>softplus</td>\n      <td>110</td>\n      <td>200</td>\n      <td>0.990364</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>softplus</td>\n      <td>120</td>\n      <td>200</td>\n      <td>0.990709</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>softplus</td>\n      <td>130</td>\n      <td>200</td>\n      <td>0.991264</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>softplus</td>\n      <td>140</td>\n      <td>200</td>\n      <td>0.991197</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>softplus</td>\n      <td>150</td>\n      <td>200</td>\n      <td>0.991263</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>softplus</td>\n      <td>160</td>\n      <td>200</td>\n      <td>0.991056</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>softplus</td>\n      <td>170</td>\n      <td>200</td>\n      <td>0.991857</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>softplus</td>\n      <td>180</td>\n      <td>200</td>\n      <td>0.995169</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>softplus</td>\n      <td>190</td>\n      <td>200</td>\n      <td>0.994680</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>softplus</td>\n      <td>200</td>\n      <td>200</td>\n      <td>0.996222</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(params), pd.DataFrame({'model__first_layer_nodes': [200] * len(params)}),pd.DataFrame(means, columns=['R2'])], axis =1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T01:03:01.661699400Z",
     "start_time": "2023-12-27T01:03:01.589036200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Merge all files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 G:\\Chamodi\\Machine_Learning\\ANN_1D\\Pressure_hyperparameter_tuning_2layers_nodrop_noscale\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "#Access input folder\n",
    "input_dir = Path (current_dir.parent/\"Pressure_hyperparameter_tuning_2layers_nodrop_noscale\")\n",
    "print (\"1\",input_dir)\n",
    "\n",
    "# Output Excel file\n",
    "output_excel_file = Path(current_dir.parent/\"Pressure_hyperparameter_tuning_2layers_nodrop_noscale/S1_summary.xlsx\")\n",
    "\n",
    "# List to store DataFrames from CSV files\n",
    "dfs = []\n",
    "\n",
    "# Loop through CSV files in the directory\n",
    "for csv_file in input_dir.glob('*.csv'):\n",
    "    # Read CSV file into a DataFrame and append to the list\n",
    "    df = pd.read_csv(csv_file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate DataFrames in the list along rows\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Write the merged DataFrame to an Excel file\n",
    "merged_df.to_excel(output_excel_file, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T01:07:06.320491300Z",
     "start_time": "2023-12-27T01:07:05.747582100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### S2 - Hyperparameter tuning - batch size, epoch, optimizer, learning rate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "# Set seed for NumPy\n",
    "np.random.seed(71)\n",
    "\n",
    "# Set seed for TensorFlow\n",
    "tf.random.set_seed(71)\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model2():\n",
    " # create model\n",
    " model = Sequential()\n",
    " model.add(Dense(units=200, input_shape=(X_train.shape[1],), activation='softplus'))\n",
    " #model.add(Dropout(0.1))\n",
    " model.add(Dense(units=200, activation='softplus'))\n",
    " #model.add(Dropout(0.1))\n",
    " model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    " return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T00:01:00.089049600Z",
     "start_time": "2024-02-06T00:01:00.065059500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "Best: 0.998860 using {'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.01}\n",
      "0.960343 (0.018466) with: {'batch_size': 30, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.001}\n",
      "0.998754 (0.000939) with: {'batch_size': 30, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.01}\n",
      "0.980541 (0.024504) with: {'batch_size': 30, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.1}\n",
      "0.964397 (0.018068) with: {'batch_size': 30, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.nadam.Nadam'>, 'optimizer__learning_rate': 0.001}\n",
      "0.998411 (0.001020) with: {'batch_size': 30, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.nadam.Nadam'>, 'optimizer__learning_rate': 0.01}\n",
      "0.996235 (0.004526) with: {'batch_size': 30, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.nadam.Nadam'>, 'optimizer__learning_rate': 0.1}\n",
      "0.956833 (0.017624) with: {'batch_size': 30, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'optimizer__learning_rate': 0.001}\n",
      "0.991713 (0.007547) with: {'batch_size': 30, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'optimizer__learning_rate': 0.01}\n",
      "0.978488 (0.018163) with: {'batch_size': 30, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'optimizer__learning_rate': 0.1}\n",
      "0.942443 (0.023501) with: {'batch_size': 40, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.001}\n",
      "0.995988 (0.003195) with: {'batch_size': 40, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.01}\n",
      "0.946236 (0.101372) with: {'batch_size': 40, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.1}\n",
      "0.948785 (0.021430) with: {'batch_size': 40, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.nadam.Nadam'>, 'optimizer__learning_rate': 0.001}\n",
      "0.998299 (0.001530) with: {'batch_size': 40, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.nadam.Nadam'>, 'optimizer__learning_rate': 0.01}\n",
      "0.997139 (0.002291) with: {'batch_size': 40, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.nadam.Nadam'>, 'optimizer__learning_rate': 0.1}\n",
      "0.944719 (0.021103) with: {'batch_size': 40, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'optimizer__learning_rate': 0.001}\n",
      "0.976442 (0.025718) with: {'batch_size': 40, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'optimizer__learning_rate': 0.01}\n",
      "0.838285 (0.173285) with: {'batch_size': 40, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'optimizer__learning_rate': 0.1}\n",
      "0.935857 (0.025544) with: {'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.001}\n",
      "0.998860 (0.000390) with: {'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.01}\n",
      "0.993624 (0.004581) with: {'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.1}\n",
      "0.940932 (0.021848) with: {'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.nadam.Nadam'>, 'optimizer__learning_rate': 0.001}\n",
      "0.998144 (0.001380) with: {'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.nadam.Nadam'>, 'optimizer__learning_rate': 0.01}\n",
      "0.996263 (0.002622) with: {'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.nadam.Nadam'>, 'optimizer__learning_rate': 0.1}\n",
      "0.942783 (0.018509) with: {'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'optimizer__learning_rate': 0.001}\n",
      "0.962729 (0.043562) with: {'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'optimizer__learning_rate': 0.01}\n",
      "0.929608 (0.076168) with: {'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'optimizer__learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model2 = KerasRegressor(model=create_model2, verbose=0, random_state = 71, loss = 'mean_squared_error', metrics = ['mae'])\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [30,40,50]\n",
    "optimizer = [Adam, Nadam, RMSprop]\n",
    "learning_rate = [ 0.001,0.01, 0.1]\n",
    "epochs = [200]\n",
    "\n",
    "# gridsearch\n",
    "param_grid2 = dict(batch_size=batch_size, optimizer=optimizer, optimizer__learning_rate = learning_rate, epochs = epochs)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=71)\n",
    "grid2 = GridSearchCV(estimator=model2, param_grid=param_grid2, n_jobs=-1, scoring = 'r2', cv=kf)\n",
    "grid_result2 = grid2.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result2.best_score_, grid_result2.best_params_))\n",
    "means = grid_result2.cv_results_['mean_test_score']\n",
    "stds = grid_result2.cv_results_['std_test_score']\n",
    "params = grid_result2.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T00:02:26.561939700Z",
     "start_time": "2024-02-06T00:01:02.319566500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "    batch_size  epochs                                       optimizer  \\\n0           30     200        <class 'keras.src.optimizers.adam.Adam'>   \n1           30     200        <class 'keras.src.optimizers.adam.Adam'>   \n2           30     200        <class 'keras.src.optimizers.adam.Adam'>   \n3           30     200      <class 'keras.src.optimizers.nadam.Nadam'>   \n4           30     200      <class 'keras.src.optimizers.nadam.Nadam'>   \n5           30     200      <class 'keras.src.optimizers.nadam.Nadam'>   \n6           30     200  <class 'keras.src.optimizers.rmsprop.RMSprop'>   \n7           30     200  <class 'keras.src.optimizers.rmsprop.RMSprop'>   \n8           30     200  <class 'keras.src.optimizers.rmsprop.RMSprop'>   \n9           40     200        <class 'keras.src.optimizers.adam.Adam'>   \n10          40     200        <class 'keras.src.optimizers.adam.Adam'>   \n11          40     200        <class 'keras.src.optimizers.adam.Adam'>   \n12          40     200      <class 'keras.src.optimizers.nadam.Nadam'>   \n13          40     200      <class 'keras.src.optimizers.nadam.Nadam'>   \n14          40     200      <class 'keras.src.optimizers.nadam.Nadam'>   \n15          40     200  <class 'keras.src.optimizers.rmsprop.RMSprop'>   \n16          40     200  <class 'keras.src.optimizers.rmsprop.RMSprop'>   \n17          40     200  <class 'keras.src.optimizers.rmsprop.RMSprop'>   \n18          50     200        <class 'keras.src.optimizers.adam.Adam'>   \n19          50     200        <class 'keras.src.optimizers.adam.Adam'>   \n20          50     200        <class 'keras.src.optimizers.adam.Adam'>   \n21          50     200      <class 'keras.src.optimizers.nadam.Nadam'>   \n22          50     200      <class 'keras.src.optimizers.nadam.Nadam'>   \n23          50     200      <class 'keras.src.optimizers.nadam.Nadam'>   \n24          50     200  <class 'keras.src.optimizers.rmsprop.RMSprop'>   \n25          50     200  <class 'keras.src.optimizers.rmsprop.RMSprop'>   \n26          50     200  <class 'keras.src.optimizers.rmsprop.RMSprop'>   \n\n    optimizer__learning_rate        R2  \n0                      0.001  0.960343  \n1                      0.010  0.998754  \n2                      0.100  0.980541  \n3                      0.001  0.964397  \n4                      0.010  0.998411  \n5                      0.100  0.996235  \n6                      0.001  0.956833  \n7                      0.010  0.991713  \n8                      0.100  0.978488  \n9                      0.001  0.942443  \n10                     0.010  0.995988  \n11                     0.100  0.946236  \n12                     0.001  0.948785  \n13                     0.010  0.998299  \n14                     0.100  0.997139  \n15                     0.001  0.944719  \n16                     0.010  0.976442  \n17                     0.100  0.838285  \n18                     0.001  0.935857  \n19                     0.010  0.998860  \n20                     0.100  0.993624  \n21                     0.001  0.940932  \n22                     0.010  0.998144  \n23                     0.100  0.996263  \n24                     0.001  0.942783  \n25                     0.010  0.962729  \n26                     0.100  0.929608  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>batch_size</th>\n      <th>epochs</th>\n      <th>optimizer</th>\n      <th>optimizer__learning_rate</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n      <td>0.001</td>\n      <td>0.960343</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n      <td>0.010</td>\n      <td>0.998754</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n      <td>0.100</td>\n      <td>0.980541</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.nadam.Nadam'&gt;</td>\n      <td>0.001</td>\n      <td>0.964397</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.nadam.Nadam'&gt;</td>\n      <td>0.010</td>\n      <td>0.998411</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>30</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.nadam.Nadam'&gt;</td>\n      <td>0.100</td>\n      <td>0.996235</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>30</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.rmsprop.RMSprop'&gt;</td>\n      <td>0.001</td>\n      <td>0.956833</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>30</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.rmsprop.RMSprop'&gt;</td>\n      <td>0.010</td>\n      <td>0.991713</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>30</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.rmsprop.RMSprop'&gt;</td>\n      <td>0.100</td>\n      <td>0.978488</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>40</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n      <td>0.001</td>\n      <td>0.942443</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>40</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n      <td>0.010</td>\n      <td>0.995988</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>40</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n      <td>0.100</td>\n      <td>0.946236</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>40</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.nadam.Nadam'&gt;</td>\n      <td>0.001</td>\n      <td>0.948785</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>40</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.nadam.Nadam'&gt;</td>\n      <td>0.010</td>\n      <td>0.998299</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>40</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.nadam.Nadam'&gt;</td>\n      <td>0.100</td>\n      <td>0.997139</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>40</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.rmsprop.RMSprop'&gt;</td>\n      <td>0.001</td>\n      <td>0.944719</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>40</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.rmsprop.RMSprop'&gt;</td>\n      <td>0.010</td>\n      <td>0.976442</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>40</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.rmsprop.RMSprop'&gt;</td>\n      <td>0.100</td>\n      <td>0.838285</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>50</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n      <td>0.001</td>\n      <td>0.935857</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>50</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n      <td>0.010</td>\n      <td>0.998860</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>50</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n      <td>0.100</td>\n      <td>0.993624</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>50</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.nadam.Nadam'&gt;</td>\n      <td>0.001</td>\n      <td>0.940932</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>50</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.nadam.Nadam'&gt;</td>\n      <td>0.010</td>\n      <td>0.998144</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>50</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.nadam.Nadam'&gt;</td>\n      <td>0.100</td>\n      <td>0.996263</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>50</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.rmsprop.RMSprop'&gt;</td>\n      <td>0.001</td>\n      <td>0.942783</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>50</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.rmsprop.RMSprop'&gt;</td>\n      <td>0.010</td>\n      <td>0.962729</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>50</td>\n      <td>200</td>\n      <td>&lt;class 'keras.src.optimizers.rmsprop.RMSprop'&gt;</td>\n      <td>0.100</td>\n      <td>0.929608</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(params), pd.DataFrame(means, columns=['R2'])], axis =1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T00:02:57.277541500Z",
     "start_time": "2024-02-06T00:02:57.241783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 50, 'epochs': 200, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'optimizer__learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_result2.best_params_\n",
    "print (best_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T00:03:00.268781700Z",
     "start_time": "2024-02-06T00:03:00.240758200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the final model using the best hyperparameters\n",
    "final_model = KerasRegressor(\n",
    "    build_fn=create_model2,\n",
    "    verbose=0,\n",
    "    random_state=71,\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae'],\n",
    "    optimizer = Nadam,\n",
    "    optimizer__learning_rate = 0.01\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   mode='min',\n",
    "                   patience=50,\n",
    "                   restore_best_weights = True)\n",
    "# Train the final model on the entire training set\n",
    "history = final_model.fit(X_train, y_train,\n",
    "                validation_data = (X_val, y_val),\n",
    "                callbacks=[es],\n",
    "                epochs=best_params['epochs'],\n",
    "                batch_size=best_params['batch_size'],\n",
    "                verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyse learn history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's see the training and validation accuracy by epoch\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss'] # you can change this\n",
    "val_loss_values = history_dict['val_loss'] # you can also change this\n",
    "epochs = range(1, len(loss_values) + 1) # range of X (no. of epochs)\n",
    "plt.plot(epochs, loss_values, 'blue', label='Train set loss')\n",
    "plt.plot(epochs, val_loss_values, 'orange', label='Validation set loss')\n",
    "#plt.title('Training and testing loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_values1 = history_dict['mae'] # you can change this\n",
    "val_loss_values1 = history_dict['val_mae'] # you can also change this\n",
    "epochs = range(1, len(loss_values1) + 1) # range of X (no. of epochs)\n",
    "plt.plot(epochs, loss_values1, 'blue', label='Train set MAE')\n",
    "plt.plot(epochs, val_loss_values1, 'orange', label='Validation set MAE')\n",
    "#plt.title('Training and testing MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Artificial Neural Network",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
